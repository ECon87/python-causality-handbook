
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>19 - Building a Causal Model &#8212; Causal Inference for the Brave and True</title>
    
  <link rel="stylesheet" href="_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/sphinx-book-theme.2d2078699c18a0efb88233928e1cf6ed.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="20 - Evaluating Causal Models" href="20-Evaluating-Causal-Models.html" />
    <link rel="prev" title="18 - When Prediction Fails" href="18-When-Prediction-Fails.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  
  <h1 class="site-logo" id="site-title">Causal Inference for the Brave and True</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="landing-page.html">
   Causal Inference for The Brave and True
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Part I - The Yang
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01-Introduction-To-Causality.html">
   01 - Introduction To Causality
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02-Randomised-Experiments.html">
   02 - Randomised Experiments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-Stats-Review-The-Most-Dangerous-Equation.html">
   03 - Stats Review: The Most Dangerous Equation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-Graphical-Causal-Models.html">
   04 - Graphical Causal Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-The-Unreasonable-Effectiveness-of-Linear-Regression.html">
   05 - The Unreasonable Effectiveness of Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-Grouped-and-Dummy-Regression.html">
   06 - Grouped and Dummy Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-Beyond-Confounders.html">
   07 - Beyond Confounders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-Instrumental-Variables.html">
   08 - Instrumental Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-Non-Compliance-and-LATE.html">
   09 - Non Compliance and LATE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-Matching.html">
   10 - Matching
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11-Propensity-Score.html">
   11 - Propensity Score
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12-Doubly-Robust-Estimation.html">
   12 - Doubly Robust Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13-Panel-Data-and-Fixed-Effects.html">
   13 - Panel Data and Fixed Effects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14-Difference-in-Difference.html">
   14 - Difference-in-Difference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15-Synthetic-Control.html">
   15 - Synthetic Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16-Regression-Discontinuity-Design.html">
   16 - Regression Discontinuity Design
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Part II - The Yin
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="17-Predictive-Models-101.html">
   17 - Predictive Models 101
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="18-When-Prediction-Fails.html">
   18 - When Prediction Fails
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   19 - Building a Causal Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="20-Evaluating-Causal-Models.html">
   20 - Evaluating Causal Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="21-Debiasing-with-Orthogonalization.html">
   21 - Debiasing with Orthogonalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="22-Debiasing-with-Propensity-Score.html">
   22 - Debiasing with Propensity Score
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="23-Plug-and-Play-Estimators.html">
   23 - Plug-and-Play Estimators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="24-Meta-Learners.html">
   24 - Meta Learners
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="25-Debiased-Orthogonal-Machine-Learning.html">
   25 - Debiased/Orthogonal Machine Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Contribute
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference external" href="https://www.patreon.com/causal_inference_for_the_brave_and_true">
   Patreon
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/19-Causal-Models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/matheusfacure/python-causality-handbook/issues/new?title=Issue%20on%20page%20%2F19-Causal-Models.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/matheusfacure/python-causality-handbook/master?urlpath=tree/19-Causal-Models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-ate-to-cate">
   From ATE to CATE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#predicting-elasticity">
   Predicting Elasticity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-ideas">
   Key Ideas
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contribute">
   Contribute
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="building-a-causal-model">
<h1>19 - Building a Causal Model<a class="headerlink" href="#building-a-causal-model" title="Permalink to this headline">¶</a></h1>
<div class="section" id="from-ate-to-cate">
<h2>From ATE to CATE<a class="headerlink" href="#from-ate-to-cate" title="Permalink to this headline">¶</a></h2>
<p>It’s a bit strange to name a chapter Building Causal Models this far out in the book. If not building causal models, what have we been doing up until this point? Fair enough. Perhaps a better name would be Building Conditional Average Treatment Effect models, but that’s just too long. Let me clarify. In the first part of the book, we’ve focused on estimating the Average Treatment Effect (ATE)</p>
<div class="math notranslate nohighlight">
\[
E[Y_1−Y_0]
\]</div>
<p>or the continuous treatment equivalent</p>
<div class="math notranslate nohighlight">
\[
E[y'(t)]
\]</div>
<p>where \(y’(t)\) is the treatment derivative of the response function or outcome. We’ve learned techniques to uncover the general effectiveness of a treatment. It’s something useful to have when the type of decision making problem you have is binary: to treat or not to treat. Now, on part two, we will move to a finer analysis of the treatment effect. I’ve been hinting on it now and then, but it’s time we make it explicit. Here, we will focus on estimating the Conditional Average Treatment Effect (CATE)</p>
<div class="math notranslate nohighlight">
\[
E[Y_1−Y_0 | X] \ \text{or} \ E[y'(t)|X]
\]</div>
<p>The conditioning on \(X\) means that we now allow the treatment effect to be different depending on the characteristics of each unit. As I’ve said before, the CATE is better suited when we want to do personalisation. Here, we believe that not all entities respond equally well to the treatment. We also want to leverage that heterogeneity. We want to treat only the right units (in the binary case) or figure out what is the optimal treatment dosage for each unit (in the continuous case).</p>
<p>For instance, if you are a bank that has to decide the loan each customer is eligible for, you can be damn sure that it’s not a good idea to give loads of money to everyone - although it might be reasonable for some. You will have to be smart with your treatment (loan amount). Perhaps, depending on the customer credit score (\(X\)), you can figure out what is the proper loan dosage. Of course, you don’t need to be a big institution to leverage personalisation. There’s no shortage of examples where it applies. What days of the year should you do sales? How much should you charge for whatever product? How much exercise is too much exercise for each person?</p>
<p>In the previous chapter, we went through the example of figuring out how much discount we should give for each individual customer. We tried using predictive machine learning for that task, but things didn’t go so well. Prediction ended up hindering our ability to estimate the CATE. It’s a good idea to recap what we saw then so we can come up with some idea on how to fix it.</p>
<p>To better grasp the problem, you can think about the units (be that customers, days, stores) as points in a Treatment and Outcome plot.</p>
<p><img alt="img" src="_images/customers.png" /></p>
<p>Your intuition says that there is probably some sort of personalisation you can do here. For instance, suppose \(Y\) here is sales and \(T\) is how much discount is given to a customer. Sure, you could give everyone the same discount on a product, but it is probably the case that there are people for whom the discount would be more effective. Some customers would buy the product at full price without a problem. Others might only buy it with a discount. Making this intuition more precise, we could say that you think there are units that are more responsive to the treatment than others. If that is the case, you want to partition your unit space in such a way that the high responsiveness  units end up together and, at the same time, separates them from the units with low responsiveness.</p>
<p>The problem with the prediction model is that it essentially partitions the space on the thing it predicts, that is, on the outcome \(Y\).</p>
<p><img alt="img" src="_images/y-partition.png" /></p>
<p>By definition, the goal of prediction is to produce a new dimension, \(\hat{Y}\), where the thing you are trying to predict doesn’t change much when we hold that dimension fixed. If you make a model that predicts sales and you look at groups of units with the same sales prediction, the real or observed sales shouldn’t change much in that group, or else your prediction model sucks.</p>
<p>Here is another way of putting it: looking at partitions defined by a prediction model of \(Y\), by definition, lowers the range in which \(Y\) is allowed to vary. And that’s a huge problem if your goal is not prediction, but understanding how \(Y\) changes with \(T\). Because if the changes in \(Y\) are suddenly constrained, or if \(Y\) is suddenly held fixed by your prediction, you won’t see it changing once you move \(T\).</p>
<p><img alt="img" src="_images/y-split.png" /></p>
<p>This is what is shown in the image above. Once you partition your data on the outcome axis, you forcibly make it so that \(Y\) can’t change much if you move \(T\), which hinders your ability to know how \(T\) affects \(Y\).</p>
<p>Of course this is a rather dramatized case, because in real life you almost never predict \(Y\) that well, so it will vary even for groups defined by the same prediction. But the central idea remains: focusing on prediction does not move you in the right direction when your goal is to estimate elasticity, that is, \(\frac{\delta Y}{ \delta T}\).</p>
<p>So what does? Here is an idea. What if instead of predicting \(Y\), we tried to predict the derivative \(\frac{\delta Y}{ \delta T}\). If we could manage to do that, what would hopefully happen is that we would split the space somewhat like the following image.</p>
<p><img alt="img" src="_images/elast-partition.png" /></p>
<p>That would be wonderful because now we would be able to estimate different elasticities on each partition. And notice here that the elasticity is just the slope of the line or function that goes from \(T\) to \(Y\). So, if we can produce partitions where the slope or elasticity differs, it means that entities on those partitions have different responsiveness to the treatment. Then we can personalise.</p>
<p><img alt="img" src="_images/elast-split.png" /></p>
<p>In other words, what you want is to move away from predicting \(Y\) in its raw form and start to predict the derivative of \(Y\) on \(T\),  \(\frac{\delta Y}{ \delta T}\). For example, suppose that \(Y\) is ice cream sales, \(T\) is ice cream price and each unit \(i\) is a day. Let’s set moral issues aside, for the sake of the argument, and pretend that you can change the price of ice cream every day. If you can somehow find the days where \(\frac{\delta Sales}{ \delta Price}\) <strong>is low</strong>, that means you can increase prices without losing much sales on those days. Perhaps you do this already, say, when you increase them on holiday season. The point being, it’s useful to differentiate days in terms of the price elasticity because it gives you some basis on how to set prices in an optimal way.</p>
<p>Ok, you might say, but this is kind of tricky. How can I predict elasticity \(\frac{\delta Sales}{ \delta Price}\) if I can’t see it. That’s a very good point. Elasticity is essentially non observable on a unit level. Not only that, it’s a strange concept. We are much more accustomed to thinking in terms of raw quantities rather than in terms of change rates of those same quantities. So, to conceptualize elasticity better, here is a little trick. You can think about each entity as having a \(Y_i\) value, sales in our example, but also an individual elasticity \(\frac{delta Y_i}{\delta T_i}\). The elasticity is how much \(Y\) changes with \(T\), so you can think about each entity also having a slope coefficient associated to it \(\frac{\delta Y}{ \delta T}_i\). In our example, we would say each day has a slope coefficient of price on sales.</p>
<p><img alt="img" src="_images/elasticity.png" /></p>
<p>Of course, we can’t see those individual slope coefficients. For us to see the individual slopes, we would had to observe each day under two different prices and calculate how the sales changes for each of those prices</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta Y_i}{ \delta T_i} \approx \frac{Y(T_i) - Y(T_i + \epsilon)}{T_i - (T_i + \epsilon)}
\]</div>
<p>This is the fundamental problem of causal inference all over again. We can’t ever see the same unite under different treatment conditions.</p>
</div>
<div class="section" id="predicting-elasticity">
<h2>Predicting Elasticity<a class="headerlink" href="#predicting-elasticity" title="Permalink to this headline">¶</a></h2>
<p>We got to a complicated situation here. We argued that predicting \(Y\) would actually hinder our capacity of figuring out how to impact \(Y\) by playing with \(T\). Rather, to solve that problem, we would have to predict the elasticity \(\frac{\delta Y_i}{ \delta T_i}\) directly. Sadly, we can’t observe elasticity, so what can we do to predict it???</p>
<p>Here is another idea. What if we use a linear regression?</p>
<p><img alt="img" src="_images/linear-fix.png" /></p>
<p>Let’s say you fit the following linear model to your data.</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \beta_1 t_i + \beta_2 X_i + e_i
\]</div>
<p>If you differentiate it on the treatment, you will end up with</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta y_i}{\delta t_i} = \beta_1 
\]</div>
<p>And since you can estimate the model above to get \(\hat{\beta_1}\), we might even be as bold as to say that <strong>you can predict elasticity even though you can’t observe it</strong>. In the case above, it is a rather simple prediction, that is, we are predicting the constant vale \(\hat{\beta_1}\) for everyone. That’s the ATE, not the CATE. This doesn’t help us in our task of grouping entities according to how responsive they are to the treatment, simply because everyone gets the same elasticity prediction. However, we can do the following change</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \beta_1 t_i + \beta_2 X_i + \beta_3 t_i X_i  + e_i
\]</div>
<p>Which would give us the follow elasticity prediction</p>
<div class="math notranslate nohighlight">
\[
\widehat{\frac{\delta y_i}{\delta t_i}} = \hat{\beta_1} + \hat{\beta_3}X_i
\]</div>
<p>Where \(beta_3\) is a vector coefficient for the features in \(X\).</p>
<p>Now each entity defined by a different \(X_i\) will have a different elasticity prediction. In other words, the elasticity prediction will change if the features \(X\) changes. In other words, the model above gives us a way of estimating the CATE \(E[y’(t)|X]\).</p>
<p>We are finally getting somewhere. The model above allows us to make an elasticity prediction for each of our entities. With those predictions we can make more useful groups. We can take the units with high predicted elasticity and group them together. We can do the same with the ones that have low predicted elasticity. Finally, with our elasticity predictions, we can group entities by how much we think they will respond to the treatment.</p>
<p>Enough of theory for now. It’s time to walk through an example of how to make this sort of elasticity model. Let’s consider our ice cream example. Each unit \(i\) is a day. For each day, we know if it’s a weekday or not, what was the cost we had to make the ice cream (you can think of cost as a proxy for quality) and the average temperature for that day. Those will be our feature space \(X\). Then, we have our treatment, price, and our outcome, the number of ice cream sold. For this example, we will consider that the treatment is randomized, just so that we don’t have to worry with bias for now.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prices_rnd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./data/ice_cream_sales_rnd.csv&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prices_rnd</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">prices_rnd</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5000, 5)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25.8</td>
      <td>1</td>
      <td>0.3</td>
      <td>7</td>
      <td>230</td>
    </tr>
    <tr>
      <th>1</th>
      <td>22.7</td>
      <td>3</td>
      <td>0.5</td>
      <td>4</td>
      <td>190</td>
    </tr>
    <tr>
      <th>2</th>
      <td>33.7</td>
      <td>7</td>
      <td>1.0</td>
      <td>5</td>
      <td>237</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23.0</td>
      <td>4</td>
      <td>0.5</td>
      <td>5</td>
      <td>193</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.4</td>
      <td>1</td>
      <td>1.0</td>
      <td>3</td>
      <td>252</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Notice that now we don’t have simulated data. Simulated data helped us understand the problems with prediction models, but we need to go back to reality now. But if we don’t have simulated data, how on earth are we going to know if our model is good at predicting elasticity if we can’t observe elasticity? Fair point, but I’ll ask you to have a little patience here. I will dedicate an entire chapter to that. For now, let’s say that we don’t need to see the elasticity to evaluate our model. We just need to check how useful it is for segmenting our customer. Since usefulness is also prone to overfitting we now have to split our dataset into a training and a testing sample, just like we do with prediction models. We can then estimate the model on the first set and make <strong>elasticity</strong> predictions on the last.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">prices_rnd</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The first model we will consider is the following linear model</p>
<div class="math notranslate nohighlight">
\[
sales_i = \beta_0 + \beta_1 price_i + \pmb{\beta_2}X_i + e_i
\]</div>
<p>If we inspect the parameters of this model, we can see what our predicted elasticity will look like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m1</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;sales ~ price + temp+C(weekday)+cost&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">m1</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>  186.7113</td> <td>    1.770</td> <td>  105.499</td> <td> 0.000</td> <td>  183.241</td> <td>  190.181</td>
</tr>
<tr>
  <th>C(weekday)[T.2]</th> <td>  -25.0512</td> <td>    0.924</td> <td>  -27.114</td> <td> 0.000</td> <td>  -26.863</td> <td>  -23.240</td>
</tr>
<tr>
  <th>C(weekday)[T.3]</th> <td>  -24.5834</td> <td>    0.901</td> <td>  -27.282</td> <td> 0.000</td> <td>  -26.350</td> <td>  -22.817</td>
</tr>
<tr>
  <th>C(weekday)[T.4]</th> <td>  -24.3807</td> <td>    0.897</td> <td>  -27.195</td> <td> 0.000</td> <td>  -26.138</td> <td>  -22.623</td>
</tr>
<tr>
  <th>C(weekday)[T.5]</th> <td>  -24.9036</td> <td>    0.894</td> <td>  -27.850</td> <td> 0.000</td> <td>  -26.657</td> <td>  -23.150</td>
</tr>
<tr>
  <th>C(weekday)[T.6]</th> <td>  -24.0921</td> <td>    0.903</td> <td>  -26.693</td> <td> 0.000</td> <td>  -25.862</td> <td>  -22.323</td>
</tr>
<tr>
  <th>C(weekday)[T.7]</th> <td>   -0.8635</td> <td>    0.888</td> <td>   -0.972</td> <td> 0.331</td> <td>   -2.605</td> <td>    0.878</td>
</tr>
<tr>
  <th>price</th>           <td>   -2.7515</td> <td>    0.106</td> <td>  -25.970</td> <td> 0.000</td> <td>   -2.959</td> <td>   -2.544</td>
</tr>
<tr>
  <th>temp</th>            <td>    1.9848</td> <td>    0.060</td> <td>   33.117</td> <td> 0.000</td> <td>    1.867</td> <td>    2.102</td>
</tr>
<tr>
  <th>cost</th>            <td>    4.4718</td> <td>    0.528</td> <td>    8.462</td> <td> 0.000</td> <td>    3.436</td> <td>    5.508</td>
</tr>
</table></div></div>
</div>
<p>For \(m1\), the predicted price elasticity \(\widehat{\dfrac{\delta y_i}{\delta t_i}}\) will be given by \(\hat{\beta_1}\), which is -2.75, in our case. This means that for each additional BRL we charge for our ice cream, we should expect sales to go down by about 3 units.
Notice how this \(m1\) predicts the exact same elasticity for everyone. Hence, it is not a very good model if we want to know on which days are people less sensitive to ice cream price increases. It estimates the ATE when what we need here is the CATE. Remember that our goal is to partition the entities in such a way that we can personalise and optimise our treatment (price) for each individual partition. If every prediction is the same, there is no partitioning we can make. To correct for that, consider our second model:</p>
<div class="math notranslate nohighlight">
\[
sales_i = \beta_0 + \beta_1 price_i + \beta_2 price_i * temp_i * + \pmb{\beta_3}X_i + e_i
\]</div>
<p>This second model includes an interaction term between price and temperature. This means that it allows the elasticity to differ for different temperatures.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m2</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;sales ~ price*temp + C(weekday) + cost&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">m2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>  192.4767</td> <td>    4.371</td> <td>   44.037</td> <td> 0.000</td> <td>  183.907</td> <td>  201.046</td>
</tr>
<tr>
  <th>C(weekday)[T.2]</th> <td>  -25.0805</td> <td>    0.924</td> <td>  -27.143</td> <td> 0.000</td> <td>  -26.892</td> <td>  -23.269</td>
</tr>
<tr>
  <th>C(weekday)[T.3]</th> <td>  -24.5871</td> <td>    0.901</td> <td>  -27.290</td> <td> 0.000</td> <td>  -26.354</td> <td>  -22.821</td>
</tr>
<tr>
  <th>C(weekday)[T.4]</th> <td>  -24.4225</td> <td>    0.897</td> <td>  -27.231</td> <td> 0.000</td> <td>  -26.181</td> <td>  -22.664</td>
</tr>
<tr>
  <th>C(weekday)[T.5]</th> <td>  -24.8953</td> <td>    0.894</td> <td>  -27.844</td> <td> 0.000</td> <td>  -26.648</td> <td>  -23.142</td>
</tr>
<tr>
  <th>C(weekday)[T.6]</th> <td>  -24.1269</td> <td>    0.903</td> <td>  -26.726</td> <td> 0.000</td> <td>  -25.897</td> <td>  -22.357</td>
</tr>
<tr>
  <th>C(weekday)[T.7]</th> <td>   -0.8581</td> <td>    0.888</td> <td>   -0.966</td> <td> 0.334</td> <td>   -2.599</td> <td>    0.883</td>
</tr>
<tr>
  <th>price</th>           <td>   -3.6299</td> <td>    0.618</td> <td>   -5.873</td> <td> 0.000</td> <td>   -4.842</td> <td>   -2.418</td>
</tr>
<tr>
  <th>temp</th>            <td>    1.7459</td> <td>    0.176</td> <td>    9.912</td> <td> 0.000</td> <td>    1.401</td> <td>    2.091</td>
</tr>
<tr>
  <th>price:temp</th>      <td>    0.0366</td> <td>    0.025</td> <td>    1.443</td> <td> 0.149</td> <td>   -0.013</td> <td>    0.086</td>
</tr>
<tr>
  <th>cost</th>            <td>    4.4558</td> <td>    0.529</td> <td>    8.431</td> <td> 0.000</td> <td>    3.420</td> <td>    5.492</td>
</tr>
</table></div></div>
</div>
<p>Once we estimate the model, the predicted elasticity is given by</p>
<div class="math notranslate nohighlight">
\[
\widehat{\frac{\delta sales_i}{\delta price_i}} = \hat{\beta_1} + \hat{\beta_3}temp_i
\]</div>
<p>Notice that \(\hat{\beta_3}\) is positive 0,03 and the baseline elasticity (the elasticity at \(0C^o\)) is -3.6. This means that, on average, as we increase price, sales go down, which makes sense. It also means that for each additional degree in temperature, people become less sensitive to price increases on ice cream (although not by much). For example, at \(25C^o\), for each additional BRL we charge, our sales go down by 2.8 units \((-3.6 + (0.03 * 25))\). But at  \(35C^o\), for each additional BRL we charge, they go down only by 2.5 units \((-3.6 + (0.03 * 35))\). This is also sort of intuitive. As the days get hotter and hotter, people are willing to pay more for ice cream.</p>
<p>We can go even further. The next model includes interaction terms on all the feature space. This means that elasticity will change with temperature, day of the week and cost.</p>
<div class="math notranslate nohighlight">
\[
sales_i = \beta_0 + \beta_1 price_i + \pmb{\beta_2 X_i}*price_i + \pmb{\beta_3}X_i + e_i
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m3</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;sales ~ price*cost + price*C(weekday) + price*temp&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>According to the above model, the individual level elasticity would be given by</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta Sales}{\delta Price} = \beta_1 + \pmb{\beta_2 X_i}
\]</div>
<p>Where \(\beta_1\) is the price coefficient and \(\pmb{\beta_2}\) is the vector for the interaction coefficients.</p>
<p>Finally, let’s see how to actually make those elasticity predictions. One way is to extract the elasticity parameters from the model and use the formula above. However, we will resort to a more general approximation. Since elasticity is nothing more than the derivative of the outcome on treatment, we can use the definition of the derivative.</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta y}{\delta t} = \dfrac{y(t+\epsilon) - y(t)}{ (t + \epsilon) - t }
\]</div>
<p>with \(\epsilon\) going to zero. We can approximate this definition by replacing \(\epsilon\) by 1.</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta y}{\delta t} \approx \hat{y}(t+1) - \hat{y}(t)
\]</div>
<p>where \(\hat{y}\) is given by our model’s predictions. In words, I’ll make two predictions with my models: one passing the original data and another passing the original data but with the treatment incremented by one unit. Below, you can see a function for doing that.</p>
<p>Since we’ve used the train set to estimate our model, we will now make predictions on the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pred_elasticity</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
        <span class="s2">&quot;pred_elast&quot;</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">t</span><span class="p">:</span><span class="n">df</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">}))</span> <span class="o">-</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="p">})</span>

<span class="n">pred_elasticity</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
      <th>pred_elast</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2648</th>
      <td>18.6</td>
      <td>7</td>
      <td>0.5</td>
      <td>10</td>
      <td>185</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>2456</th>
      <td>26.0</td>
      <td>3</td>
      <td>0.5</td>
      <td>10</td>
      <td>200</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>4557</th>
      <td>23.7</td>
      <td>3</td>
      <td>0.3</td>
      <td>8</td>
      <td>192</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>4884</th>
      <td>28.9</td>
      <td>4</td>
      <td>1.5</td>
      <td>6</td>
      <td>213</td>
      <td>-2.751463</td>
    </tr>
    <tr>
      <th>92</th>
      <td>23.7</td>
      <td>1</td>
      <td>0.5</td>
      <td>8</td>
      <td>207</td>
      <td>-2.751463</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Making elasticity predictions using \(m1\) is not much fun. We can see that it predicts the exact same value for all the days. That’s because there are no interaction terms on that model. However, if we make predictions using \(m3\), it outputs a different elasticity prediction for each day.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_elast3</span> <span class="o">=</span> <span class="n">pred_elasticity</span><span class="p">(</span><span class="n">m3</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pred_elast3</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
      <th>pred_elast</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4764</th>
      <td>31.1</td>
      <td>6</td>
      <td>1.0</td>
      <td>3</td>
      <td>212</td>
      <td>1.144309</td>
    </tr>
    <tr>
      <th>4324</th>
      <td>24.8</td>
      <td>7</td>
      <td>0.5</td>
      <td>10</td>
      <td>182</td>
      <td>-9.994303</td>
    </tr>
    <tr>
      <th>4536</th>
      <td>25.0</td>
      <td>2</td>
      <td>1.5</td>
      <td>6</td>
      <td>205</td>
      <td>0.279273</td>
    </tr>
    <tr>
      <th>3466</th>
      <td>26.0</td>
      <td>3</td>
      <td>1.5</td>
      <td>3</td>
      <td>205</td>
      <td>0.308320</td>
    </tr>
    <tr>
      <th>115</th>
      <td>19.3</td>
      <td>3</td>
      <td>0.3</td>
      <td>9</td>
      <td>177</td>
      <td>-0.349745</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Notice how the predictions are numbers that go from something like -9 to something 1. Those are not predictions of the sales column, which is in the order of the hundreds. Rather, <strong>it’s a prediction of how much sales would change if we increased price by one unit</strong>. Right out of the bet, we can see some strange numbers. For example, take a look at day 4764. It’s predicting a positive elasticity. In other words, we are predicting that sales will increase if we increase ice cream price. This doesn’t appeal to our economic sense. It’s probably the case that the model is doing some weird extrapolation on that prediction. Fortunately, you don’t have to worry too much about it. Remember that our ultimate goal is to segment the units by how sensitive they are to the treatment. It’s <strong>not</strong> to come up with the most accurate elasticity prediction ever. For our main goal, it suffices if the elasticity predictions orders the units according to how sensitive they are. In other words, even if positive elasticity predictions like 1.1, or -0.5 don’t make much sense, all we need is that the ordering is correct, that is, we want the units with prediction 1.1 to be less impacted by price increase than units with predictions -0.5.</p>
<p>Now that we have our elasticity model, let us contrast it with a purely predictive model. We will use a machine learning algorithm that uses price, temperature, weekday and cost as features \(X\) and tries to predict ice cream sales.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;temp&quot;</span><span class="p">,</span> <span class="s2">&quot;weekday&quot;</span><span class="p">,</span> <span class="s2">&quot;cost&quot;</span><span class="p">,</span> <span class="s2">&quot;price&quot;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;sales&quot;</span>
<span class="n">ml</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">()</span>
<span class="n">ml</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">X</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>

<span class="c1"># make sure the model is not overfiting.</span>
<span class="n">ml</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">X</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9124088322890126
</pre></div>
</div>
</div>
</div>
<p>This model can make predictions about how much sales we will have on each day. But is it suited for what we really want?</p>
<p>To see which model is more useful, let’s try using them for segmenting the units. For each model, we will partition the units into 2 groups. Our hope is that one group is highly responsive to price increase while the other not so much. If that is the case, we can organize our business around those groups: for the days that fall in the high responsiveness group, we better not set prices too high. For the low responsiveness group, we can increase prices without risking too much in sales.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bands_df</span> <span class="o">=</span> <span class="n">pred_elast3</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">elast_band</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">pred_elast3</span><span class="p">[</span><span class="s2">&quot;pred_elast&quot;</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">pred_sales</span> <span class="o">=</span> <span class="n">ml</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_elast3</span><span class="p">[</span><span class="n">X</span><span class="p">]),</span>
    <span class="n">pred_band</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">qcut</span><span class="p">(</span><span class="n">ml</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_elast3</span><span class="p">[</span><span class="n">X</span><span class="p">]),</span> <span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">bands_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>weekday</th>
      <th>cost</th>
      <th>price</th>
      <th>sales</th>
      <th>pred_elast</th>
      <th>elast_band</th>
      <th>pred_sales</th>
      <th>pred_band</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2648</th>
      <td>18.6</td>
      <td>7</td>
      <td>0.5</td>
      <td>10</td>
      <td>185</td>
      <td>-10.301045</td>
      <td>(-10.597999999999999, -0.00555]</td>
      <td>186.878081</td>
      <td>(161.089, 198.735]</td>
    </tr>
    <tr>
      <th>2456</th>
      <td>26.0</td>
      <td>3</td>
      <td>0.5</td>
      <td>10</td>
      <td>200</td>
      <td>0.036165</td>
      <td>(-0.00555, 1.389]</td>
      <td>203.188327</td>
      <td>(198.735, 257.746]</td>
    </tr>
    <tr>
      <th>4557</th>
      <td>23.7</td>
      <td>3</td>
      <td>0.3</td>
      <td>8</td>
      <td>192</td>
      <td>-0.132057</td>
      <td>(-10.597999999999999, -0.00555]</td>
      <td>188.800637</td>
      <td>(161.089, 198.735]</td>
    </tr>
    <tr>
      <th>4884</th>
      <td>28.9</td>
      <td>4</td>
      <td>1.5</td>
      <td>6</td>
      <td>213</td>
      <td>0.860663</td>
      <td>(-0.00555, 1.389]</td>
      <td>210.430813</td>
      <td>(198.735, 257.746]</td>
    </tr>
    <tr>
      <th>92</th>
      <td>23.7</td>
      <td>1</td>
      <td>0.5</td>
      <td>8</td>
      <td>207</td>
      <td>-9.953698</td>
      <td>(-10.597999999999999, -0.00555]</td>
      <td>209.044522</td>
      <td>(198.735, 257.746]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>I might be getting ahead of myself now, since we will only look at model evaluation on the next chapter. But I feel I can give you a taste of what it looks like. One very simple way to check how good are those partition schemas - and by good I mean useful - is to plot a regression line of prices on sales for each partition. We can achieve that easily with Seaborn’s <code class="docutils literal notranslate"><span class="pre">regplot</span></code> combined with <code class="docutils literal notranslate"><span class="pre">FacetGrid</span></code>.</p>
<p>Below, we can see the partitions made using the elasticity predictions. Remember that all of this is done in the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">bands_df</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;elast_band&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_dataframe</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;sales&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_titles</span><span class="p">(</span><span class="n">col_template</span><span class="o">=</span><span class="s2">&quot;Elast. Band </span><span class="si">{col_name}</span><span class="s2">&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/19-Causal-Models_21_0.png" src="_images/19-Causal-Models_21_0.png" />
</div>
</div>
<p>As we can see, it looks like this partitioning scheme is useful. For the first partition, there is a high price sensitivity. Sales goes down by a lot as prices go up. However, for the second partition, sales remain roughly unganged as price goes up. In fact, it even looks like sales goes up as we increase price, but that’s probably noise.</p>
<p>Contrast this with the partitions made using the ML prediction model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">bands_df</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;pred_band&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_dataframe</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;sales&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_titles</span><span class="p">(</span><span class="n">col_template</span><span class="o">=</span><span class="s2">&quot;Pred. Band </span><span class="si">{col_name}</span><span class="s2">&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/19-Causal-Models_23_0.png" src="_images/19-Causal-Models_23_0.png" />
</div>
</div>
<p>I really like this plot because it conveys a very important point. As you can see, the predictive model partitions are splitting the units on the y axis. On days like those in the first partition, we don’t sell a lot of ice cream. Moreover, we do sell more on days like those in the second partition. I find this amazing because the prediction model is doing exactly what it is supposed to do: it predicts sales. It can distinguish between days where there will be low versus high ice cream sales.</p>
<p>The only problem is that prediction is not particularly useful here. Ultimately, we want to know when we can increase prices and when we can’t. But once we look at the slopes of the lines in the predictive model partitions, we see that they don’t change much. In other words, both partitions, as defined by the prediction model, have about the same responsiveness to price increase. This doesn’t offer us much insight into which are the days we can increase prices, since it looks like price is not affecting sales at all.</p>
</div>
<div class="section" id="key-ideas">
<h2>Key Ideas<a class="headerlink" href="#key-ideas" title="Permalink to this headline">¶</a></h2>
<p>We finally formalized the concept of Conditional Average Treatment Effect and how it can be useful for personalisation. Namely, if we can understand how each unit responds to a treatment, that is, if we can understand the heterogeneity of the treatment effect, we can give the best treatment depending on the unit’s individual characteristics.</p>
<p>Then, we revisited the problem of using predictive models for that task. By doing so, we argued that we could rethink the estimation task from predicting \(Y\) in it’s raw format to predicting how \(Y\) changes with \(T\), \(\frac{\delta y}{\delta t}\).</p>
<p>Sadly, it’s not at all obvious how to build models for that. Since we can’t observe elasticity directly, it’s hard to make a model that predicts it. But linear regression came to our rescue. By using a regression model that is fitted to predict \(Y\), we found a way to also predict \(\frac{\delta y}{\delta t}\). We also had to include interaction terms of the treatment and the features. This made it so that our elasticity prediction was different for each customer. In other words, we were now estimating \(E[T’(t) | X]\). Those elasticity predictions were then used to group our units into more or less sensitive to the treatment, ultimately helping us decide the treatment level for each group.</p>
<p><img alt="img" src="_images/economists.png" /></p>
<p>One natural question that arises from all this is if we can replace the linear regression by a generic machine learning model and use that to predict elasticity. The answer is yes, but there are some caveats. This chapter used a very simple causal model because I think it’s easier to understand the concept behind them with linear regression, but we will see some more sophisticated models in the chapters to come. However, before that, I first need to cover a very important topic, which is how can we compare two causal models and decide which one is better.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>The things I’ve written here are mostly stuff from my head. I’ve learned them through experience. This means there isn’t a direct reference I can point you to. It also means that the things I wrote here have <strong>not</strong> passed the academic scrutiny that good science often goes through. Instead, notice how I’m talking about things that work in practice, but I don’t spend too much time explaining why that is the case. It’s a sort of science from the streets, if you will. However, I am putting this up for public scrutiny, so, by all means, if you find something preposterous, open an issue and I’ll address it to the best of my efforts.</p>
</div>
<div class="section" id="contribute">
<h2>Contribute<a class="headerlink" href="#contribute" title="Permalink to this headline">¶</a></h2>
<p>Causal Inference for the Brave and True is an open-source material on causal inference, the statistics of science. It uses only free software, based in Python. Its goal is to be accessible monetarily and intellectually.
If you found this book valuable and you want to support it, please go to <a class="reference external" href="https://www.patreon.com/causal_inference_for_the_brave_and_true">Patreon</a>. If you are not ready to contribute financially, you can also help by fixing typos, suggesting edits or giving feedback on passages you didn’t understand. Just go to the book’s repository and <a class="reference external" href="https://github.com/matheusfacure/python-causality-handbook/issues">open an issue</a>. Finally, if you liked this content, please share it with others who might find it useful and give it a <a class="reference external" href="https://github.com/matheusfacure/python-causality-handbook/stargazers">star on GitHub</a>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "causal-glory"
        },
        kernelOptions: {
            kernelName: "causal-glory",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'causal-glory'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="18-When-Prediction-Fails.html" title="previous page">18 - When Prediction Fails</a>
    <a class='right-next' id="next-link" href="20-Evaluating-Causal-Models.html" title="next page">20 - Evaluating Causal Models</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Matheus Facure Alves<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-97848161-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>